import os
from config import COMMON, INFER
os.environ.setdefault("MUJOCO_GL", COMMON.mujoco_gl)
import torch
import numpy as np
import metaworld
import cv2
import mujoco
import random
import torch.nn.functional as F
from src.models import LeJEPA_Robot
# --- CONFIG ---
TASK_NAME = COMMON.task_name
IMG_SIZE = COMMON.img_size
RENDER_SIZE = COMMON.render_size
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
MODEL_PATH = COMMON.model_path
CAMERA_NAME = COMMON.camera_name

# Multi-GPU (optional). In this script we call encoder/predictor directly,
# so wrap those modules (not the whole model) to actually use DataParallel.
USE_DATAPARALLEL = INFER.use_dataparallel

# Visual debugging (giá»‘ng 01_collect_data.py)
PREVIEW = INFER.preview
PREVIEW_WAIT_MS = INFER.preview_wait_ms
PREVIEW_SCALE = INFER.preview_scale

# Config MPPI (Bá»™ nÃ£o quy hoáº¡ch)
HORIZON = INFER.horizon
NUM_SAMPLES = INFER.num_samples
TEMPERATURE = INFER.temperature

# Reset/episode control (giá»‘ng 01_collect_data.py)
NUM_EPISODES = INFER.num_episodes
MAX_STEPS = INFER.max_steps

def get_env():
    ml1 = metaworld.ML1(TASK_NAME)
    env = ml1.train_classes[TASK_NAME](render_mode="rgb_array")
    return env, ml1

def chw_rgb_to_hwc_bgr(img_chw: np.ndarray) -> np.ndarray:
    """Convert CHW RGB uint8 -> HWC BGR uint8 for OpenCV display/video."""
    img_hwc = np.transpose(img_chw, (1, 2, 0))
    return cv2.cvtColor(img_hwc, cv2.COLOR_RGB2BGR)

def grab_frame_np(renderer, data, cam_name):
    """Return CHW uint8 RGB (after flip + resize)."""
    renderer.update_scene(data, camera=cam_name)
    img = renderer.render()
    img = np.flipud(img)
    img = cv2.resize(img, IMG_SIZE)
    img = np.transpose(img, (2, 0, 1))
    return img.astype(np.uint8, copy=False)

def grab_frame(renderer, data, cam_name):
    img = grab_frame_np(renderer, data, cam_name)
    return torch.tensor(img).float().to(DEVICE) / 255.0

# --- HÃ€M QUAN TRá»ŒNG: Láº¤Y áº¢NH GOAL ---
def capture_goal_image(env, renderer, cam_name):
    """
    Hack: Dá»‹ch chuyá»ƒn váº­t Ä‘áº¿n goal, chá»¥p áº£nh.
    Index chuáº©n cho Sawyer Push V3:
    - 0-6: Robot Arm
    - 7-8: Gripper (Right/Left finger)
    - 9-11: Object Position (X, Y, Z) <--- Má»¤C TIÃŠU Cá»¦A TA á»ž ÄÃ‚Y
    """
    model = env.unwrapped.model
    data = env.unwrapped.data

    # Backup tráº¡ng thÃ¡i cÅ©
    old_qpos = data.qpos.copy()
    old_qvel = data.qvel.copy()

    # Láº¥y vá»‹ trÃ­ goal
    obs_dict = env.unwrapped._get_obs_dict()
    goal_pos = obs_dict['state_desired_goal']

    # --- Sá»¬A Lá»–I: GHI VÃ€O INDEX 9:12 (Thay vÃ¬ 7:10) ---
    data.qpos[9:12] = goal_pos
    data.qvel[9:12] = 0.0

    mujoco.mj_forward(model, data)

    # Chá»¥p áº£nh goal
    img_goal_chw = grab_frame_np(renderer, data, cam_name)
    img_tensor = torch.tensor(img_goal_chw).float().to(DEVICE) / 255.0

    # Restore tráº¡ng thÃ¡i cÅ©
    data.qpos[:] = old_qpos
    data.qvel[:] = old_qvel
    mujoco.mj_forward(model, data)

    return img_tensor.unsqueeze(0), img_goal_chw  # [1, 3, 64, 64], CHW uint8

# --- MPPI CONTROLLER ---
def plan_with_mppi(model, z_curr, z_goal):
    """
    TÆ°á»Ÿng tÆ°á»£ng ra hÃ ng nghÃ¬n viá»…n cáº£nh vÃ  chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t.
    """
    B, D = z_curr.shape
    
    # 1. Random ra chuá»—i hÃ nh Ä‘á»™ng [Num_samples, Horizon, Action_dim]
    # Action space [-1, 1]
    noise = torch.randn(NUM_SAMPLES, HORIZON, 4, device=DEVICE)
    
    # Action Ä‘á» xuáº¥t (cÃ³ thá»ƒ dÃ¹ng mean cá»§a bÆ°á»›c trÆ°á»›c, á»Ÿ Ä‘Ã¢y dÃ¹ng pure noise cho Ä‘Æ¡n giáº£n)
    actions = torch.tanh(noise) # Ã‰p vá» [-1, 1]
    
    # 2. Latent Rollout (TÆ°á»Ÿng tÆ°á»£ng)
    z_states = z_curr.repeat(NUM_SAMPLES, 1) # Báº¯t Ä‘áº§u tá»« tráº¡ng thÃ¡i hiá»‡n táº¡i
    cumulative_rewards = torch.zeros(NUM_SAMPLES, device=DEVICE)
    
    for t in range(HORIZON):
        act_t = actions[:, t, :] 
        
        # Dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai: z_{t+1} = Predictor(z_t, a_t)
        z_next = model["predictor"](torch.cat([z_states, act_t], dim=1))
        
        # TÃ­nh Ä‘iá»ƒm: CÃ ng gáº§n Goal cÃ ng tá»‘t (Negative L2 Distance)
        # VÃ¬ LeJEPA há»c khÃ´ng gian Euclidean, ta dÃ¹ng L2 trá»±c tiáº¿p
        dist = torch.norm(z_next - z_goal, dim=1)
        reward = -dist 
        
        # Cá»™ng dá»“n Ä‘iá»ƒm (cÃ³ discount factor nháº¹ náº¿u muá»‘n, á»Ÿ Ä‘Ã¢y = 1)
        cumulative_rewards += reward
        z_states = z_next

    # 3. Chá»n hÃ nh Ä‘á»™ng (Weighted Average)
    # Softmax trÃªn reward Ä‘á»ƒ láº¥y trá»ng sá»‘
    scores = F.softmax(cumulative_rewards / TEMPERATURE, dim=0)
    
    # NhÃ¢n trá»ng sá»‘ vÃ o hÃ nh Ä‘á»™ng Ä‘áº§u tiÃªn
    # actions[:, 0, :] shape [N, 4]
    # scores shape [N] -> unsqueeze thÃ nh [N, 1]
    best_action = torch.sum(actions[:, 0, :] * scores.unsqueeze(1), dim=0)
    
    return best_action.detach().cpu().numpy()

# --- MAIN LOOP ---
def run_inference():
    # Load Model
    print("ðŸ§  Loading Brain...")
    base = LeJEPA_Robot(action_dim=4, z_dim=64).to(DEVICE)
    base.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    base.eval()

    # Build callables (maybe DataParallel)
    encoder = base.encoder
    predictor = base.predictor
    if USE_DATAPARALLEL and DEVICE == "cuda" and torch.cuda.device_count() > 1:
        encoder = torch.nn.DataParallel(encoder)
        predictor = torch.nn.DataParallel(predictor)
    model = {"encoder": encoder, "predictor": predictor}
    
    env, ml1 = get_env()
    model_mujoco = env.unwrapped.model 
    data = env.unwrapped.data
    renderer = mujoco.Renderer(model_mujoco, height=RENDER_SIZE[0], width=RENDER_SIZE[1])
    
    if PREVIEW:
        cv2.namedWindow("mpc_preview", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("mpc_preview", IMG_SIZE[0] * PREVIEW_SCALE * 2 + 10, IMG_SIZE[1] * PREVIEW_SCALE)

    # Loop theo episode giá»‘ng 01_collect_data.py
    for ep in range(NUM_EPISODES):
        task = random.choice(ml1.train_tasks)
        env.set_task(task)
        env.reset()

        print(f"ðŸ“¸ [ep={ep}] Capturing Goal Image...")
        img_goal, img_goal_chw = capture_goal_image(env, renderer, CAMERA_NAME)
        with torch.no_grad():
            z_goal = model["encoder"](img_goal)

        if PREVIEW:
            goal_vis = chw_rgb_to_hwc_bgr(img_goal_chw)
            if PREVIEW_SCALE and PREVIEW_SCALE != 1:
                goal_vis = cv2.resize(
                    goal_vis,
                    (IMG_SIZE[0] * PREVIEW_SCALE, IMG_SIZE[1] * PREVIEW_SCALE),
                    interpolation=cv2.INTER_NEAREST,
                )

        print(f"ðŸš€ [ep={ep}] Start Control Loop...")
        for step in range(MAX_STEPS):
            img_curr_chw = grab_frame_np(renderer, data, CAMERA_NAME)
            img_curr = torch.tensor(img_curr_chw).float().to(DEVICE).unsqueeze(0) / 255.0

            with torch.no_grad():
                z_curr = model["encoder"](img_curr)
                action = plan_with_mppi(model, z_curr, z_goal)

            action = np.clip(action, -1.0, 1.0)

            step_ret = env.step(action)
            terminated = truncated = False
            if isinstance(step_ret, (tuple, list)) and len(step_ret) == 5:
                _, _, terminated, truncated, _ = step_ret
            elif isinstance(step_ret, (tuple, list)) and len(step_ret) == 4:
                _, _, done, _ = step_ret
                terminated = bool(done)
            else:
                raise RuntimeError(f"Unexpected env.step() return: {type(step_ret)} {step_ret}")

            if PREVIEW:
                curr_vis = chw_rgb_to_hwc_bgr(img_curr_chw)
                if PREVIEW_SCALE and PREVIEW_SCALE != 1:
                    curr_vis = cv2.resize(
                        curr_vis,
                        (IMG_SIZE[0] * PREVIEW_SCALE, IMG_SIZE[1] * PREVIEW_SCALE),
                        interpolation=cv2.INTER_NEAREST,
                    )
                sep = np.zeros((curr_vis.shape[0], 10, 3), dtype=np.uint8)
                vis = np.concatenate([curr_vis, sep, goal_vis], axis=1)
                cv2.putText(
                    vis,
                    f"ep={ep} step={step}  curr | goal",
                    (10, 25),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.7,
                    (255, 255, 255),
                    2,
                    cv2.LINE_AA,
                )
                cv2.imshow("mpc_preview", vis)
                if (cv2.waitKey(PREVIEW_WAIT_MS) & 0xFF) in (ord("q"), 27):
                    return

            obs_dict = env.unwrapped._get_obs_dict()
            obj_pos = np.asarray(obs_dict["state_observation"], dtype=np.float32)[4:7]
            goal_pos = np.asarray(obs_dict["state_desired_goal"], dtype=np.float32)
            dist = float(np.linalg.norm(obj_pos - goal_pos))
            print(f"ep={ep} step={step}: Action {action[:2]}... | Dist to Goal: {dist:.4f}")

            if dist < 0.05:
                print("âœ… SUCCESSSSS! AI Ä‘Ã£ Ä‘áº©y trÃºng Ä‘Ã­ch!")
                break

            if truncated or terminated:
                break

    if PREVIEW:
        cv2.destroyAllWindows()

if __name__ == "__main__":
    run_inference()