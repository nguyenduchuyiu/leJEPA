import os
# MuJoCo default GLX often fails with errors like:
# "GLX: No GLXFBConfigs returned" / "gladLoadGL error".
# EGL works both headless and with a desktop display, so use it by default
# unless the user explicitly set MUJOCO_GL.
os.environ.setdefault("MUJOCO_GL", "egl")
import numpy as np
import metaworld
import random
import cv2
import mujoco 
from metaworld.policies import SawyerPushV3Policy
from tqdm import tqdm
from config import COMMON, COLLECT

# --- CONFIG ---
TASK_NAME = COMMON.task_name
NUM_EPISODES = COLLECT.num_episodes
MAX_STEPS = COLLECT.max_steps
IMG_SIZE = COMMON.img_size
RENDER_SIZE = COMMON.render_size
SAVE_DIR = COMMON.data_dir
CAMERA_NAME = COMMON.camera_name
NOISE_SCALE = COLLECT.noise_scale
EPSILON = COLLECT.epsilon

# For large runs (e.g. NUM_EPISODES=5000), keeping all frames in RAM and then
# calling np.savez_compressed will often trigger OOM and get the process killed.
# Stream-to-disk avoids that by writing into .npy memmaps during collection.
STREAM_TO_NPY = COLLECT.stream_to_npy

# Visual debugging:
# - If you have a display, set PREVIEW = True to see a live cv2 window.
# - On headless machines, keep PREVIEW = False.
PREVIEW = COLLECT.preview
PREVIEW_WAIT_MS = COLLECT.preview_wait_ms
PREVIEW_SCALE = COLLECT.preview_scale  # phÃ³ng to cá»­a sá»• preview (1 = giá»¯ nguyÃªn)

def get_env():
    ml1 = metaworld.ML1(TASK_NAME)
    # Render mode chá»‰ Ä‘á»ƒ cho cÃ³ thá»§ tá»¥c, ta sáº½ render thá»§ cÃ´ng
    env = ml1.train_classes[TASK_NAME](render_mode=None)
    
    task = random.choice(ml1.train_tasks)
    env.set_task(task)
    return env

# --- HÃ€M Má»šI QUAN TRá»ŒNG NHáº¤T ---
def grab_frame(renderer, data, cam_name):
    """
    DÃ¹ng mujoco.Renderer trá»±c tiáº¿p thay vÃ¬ env.render()
    """
    # 1. Cáº­p nháº­t scene vá»›i dá»¯ liá»‡u váº­t lÃ½ hiá»‡n táº¡i (data) vÃ  camera mong muá»‘n
    renderer.update_scene(data, camera=cam_name)
    
    # 2. Render ra áº£nh
    img = renderer.render() # Tráº£ vá» (H, W, 3) RGB

    # Backend tráº£ áº£nh bá»‹ láº­t dá»c
    img = np.flipud(img)
    
    # 3. Resize vá» 64x64
    img = cv2.resize(img, IMG_SIZE)
    
    # 4. Chuyá»ƒn sang Channel-First (3, 64, 64) cho PyTorch
    img = np.transpose(img, (2, 0, 1)) 
    return img

def chw_rgb_to_hwc_bgr(img_chw: np.ndarray) -> np.ndarray:
    """Convert CHW RGB uint8 -> HWC BGR uint8 for OpenCV display/video."""
    img_hwc = np.transpose(img_chw, (1, 2, 0))
    return cv2.cvtColor(img_hwc, cv2.COLOR_RGB2BGR)

# --- Sá»¬A Láº I HÃ€M COLLECT ---
def collect():
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    # Khá»Ÿi táº¡o Env
    ml1 = metaworld.ML1(TASK_NAME)
    env = ml1.train_classes[TASK_NAME](render_mode=None)

    # Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n render/policy nhÆ° cÅ©
    model = env.unwrapped.model
    data = env.unwrapped.data
    renderer = mujoco.Renderer(model, height=RENDER_SIZE[0], width=RENDER_SIZE[1])

    policy = SawyerPushV3Policy()

    max_transitions = int(NUM_EPISODES) * int(MAX_STEPS)
    write_idx = 0

    obs_mm = actions_mm = next_obs_mm = None
    data_obs = data_actions = data_next_obs = None

    if STREAM_TO_NPY:
        # Note: .npy is written as a memory-mapped array on disk (low RAM).
        obs_path = os.path.join(SAVE_DIR, "obs.npy")
        actions_path = os.path.join(SAVE_DIR, "actions.npy")
        next_obs_path = os.path.join(SAVE_DIR, "next_obs.npy")

        # grab_frame returns CHW (3, H, W). IMG_SIZE is (W, H) in cv2.resize().
        h, w = IMG_SIZE[1], IMG_SIZE[0]
        obs_mm = np.lib.format.open_memmap(obs_path, mode="w+", dtype=np.uint8, shape=(max_transitions, 3, h, w))
        actions_mm = np.lib.format.open_memmap(actions_path, mode="w+", dtype=np.float32, shape=(max_transitions, 4))
        next_obs_mm = np.lib.format.open_memmap(next_obs_path, mode="w+", dtype=np.uint8, shape=(max_transitions, 3, h, w))
    else:
        data_obs, data_actions, data_next_obs = [], [], []

    print(f"ðŸš€ Báº¯t Ä‘áº§u thu tháº­p dá»¯ liá»‡u (ÄÃ£ fix lá»—i láº·p task)...")

    if PREVIEW:
        cv2.namedWindow("preview", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("preview", IMG_SIZE[0] * PREVIEW_SCALE, IMG_SIZE[1] * PREVIEW_SCALE)

    try:
        for ep in tqdm(range(NUM_EPISODES)):
            try:
                # --- FIX 1: RANDOM TASK Má»–I Táº¬P ---
                # Pháº£i set task má»›i thÃ¬ vá»‹ trÃ­ váº­t thá»ƒ/goal má»›i thay Ä‘á»•i
                task = random.choice(ml1.train_tasks)
                env.set_task(task)

                # Reset mÃ´i trÆ°á»ng
                obs_vector, _ = env.reset()

                # Render frame Ä‘áº§u
                img_t = grab_frame(renderer, data, CAMERA_NAME)
                if PREVIEW:
                    vis = chw_rgb_to_hwc_bgr(img_t)
                    if PREVIEW_SCALE and PREVIEW_SCALE != 1:
                        vis = cv2.resize(vis, (IMG_SIZE[0] * PREVIEW_SCALE, IMG_SIZE[1] * PREVIEW_SCALE), interpolation=cv2.INTER_NEAREST)
                    cv2.imshow("preview", vis)
                    if (cv2.waitKey(PREVIEW_WAIT_MS) & 0xFF) in (ord("q"), 27):
                        raise KeyboardInterrupt

                for _ in range(MAX_STEPS):
                    if write_idx >= max_transitions:
                        break

                    # --- LOGIC MIX ACTION (Giá»¯ nguyÃªn nhÆ° Ä‘Ã£ bÃ n) ---
                    if np.random.rand() < EPSILON:
                        action = env.action_space.sample()
                    else:
                        # ÄÃ¢y lÃ  Ground Truth (Sá»± tháº­t tuyá»‡t Ä‘á»‘i)
                        obs_dict = env.unwrapped._get_obs_dict()
                        real_goal = obs_dict['state_desired_goal']

                        # --- BÆ¯á»šC 2: PHáºªU THUáº¬T VECTOR ---
                        # Ghi Ä‘Ã¨ 3 pháº§n tá»­ cuá»‘i cÃ¹ng cá»§a vector báº±ng Goal tháº­t
                        # Policy V3 sáº½ Ä‘á»c 3 sá»‘ nÃ y Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng
                        obs_vector[-3:] = real_goal
                        base_action = policy.get_action(obs_vector)
                        xyz_noise = np.random.normal(0, NOISE_SCALE, size=3)
                        gripper_noise = 0.0 # KhÃ³a gripper
                        noise = np.hstack([xyz_noise, gripper_noise])
                        action = base_action + noise

                    action = np.clip(action, -1.0, 1.0).astype(np.float32, copy=False)

                    # Step
                    obs_vector, _, _, _, _ = env.step(action)
                    img_next = grab_frame(renderer, data, CAMERA_NAME)
                    if PREVIEW:
                        vis = chw_rgb_to_hwc_bgr(img_next)
                        if PREVIEW_SCALE and PREVIEW_SCALE != 1:
                            vis = cv2.resize(vis, (IMG_SIZE[0] * PREVIEW_SCALE, IMG_SIZE[1] * PREVIEW_SCALE), interpolation=cv2.INTER_NEAREST)
                        cv2.imshow("preview", vis)
                        if (cv2.waitKey(PREVIEW_WAIT_MS) & 0xFF) in (ord("q"), 27):
                            raise KeyboardInterrupt

                    # LÆ°u
                    if STREAM_TO_NPY:
                        obs_mm[write_idx] = img_t
                        actions_mm[write_idx] = action
                        next_obs_mm[write_idx] = img_next
                    else:
                        data_obs.append(img_t)
                        data_actions.append(action)
                        data_next_obs.append(img_next)

                    write_idx += 1
                    img_t = img_next

            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"âš ï¸ Lá»—i episode {ep}: {e}")
                continue
    finally:
        # Avoid EGL errors on interpreter shutdown
        try:
            renderer.close()
        except Exception:
            pass
        if PREVIEW:
            try:
                cv2.destroyAllWindows()
            except Exception:
                pass

    if STREAM_TO_NPY:
        print("ðŸ’¾ Äang flush vÃ  lÆ°u metadata...")
        obs_mm.flush()
        actions_mm.flush()
        next_obs_mm.flush()
        meta_path = os.path.join(SAVE_DIR, "meta.npz")
        np.savez(meta_path, n=np.array([write_idx], dtype=np.int64))
        print(f"âœ… Xong! LÆ°u dáº¡ng memmap táº¡i: {SAVE_DIR} (n={write_idx})")
    else:
        print("ðŸ“¦ Äang nÃ©n vÃ  lÆ°u dá»¯ liá»‡u...")
        np_obs = np.array(data_obs, dtype=np.uint8)
        np_actions = np.array(data_actions, dtype=np.float32)
        np_next_obs = np.array(data_next_obs, dtype=np.uint8)

        save_path = os.path.join(SAVE_DIR, 'train_buffer.npz')
        np.savez_compressed(
            save_path,
            obs=np_obs,
            actions=np_actions,
            next_obs=np_next_obs
        )

        print(f"âœ… Xong! File lÆ°u táº¡i: {save_path}")
        print(f"ðŸ“Š Shape áº£nh: {np_obs.shape}")

if __name__ == "__main__":
    collect()